{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a657d-d888-4ec1-a8f0-787bd1efaeb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ea7bd2-e94e-44ff-8c01-99a1662371e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import MeCab\n",
    "import ipadic\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54370196-bbf3-4fc9-8929-c2b2eaa946ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.append('/home/jovyan/mi-drive/medinfo_lab/Research_Projects/zhang/Husky')\n",
    "sys.path.append('/home/jovyan/nas/medinfo_lab/Research_Projects/zhang/Husky')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab0cb00-9dc6-448c-b6f4-a6a8a1282d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tools.tokenizers.myMeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49544a4a-2af5-443f-99b3-757a1bf18705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tools.tokenizers import myMeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d94bcb3-f027-4983-8065-46572ed7f34b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TokenizerPath = '/home/jovyan/mi-drive/medinfo_lab/Research_Projects/zhang/Husky/kuhpTokenizer/40000_vocab/bert_bpe_vocab_40000.model'.replace(\"mi-drive\", \"nas\")\n",
    "# CorpusPath = '/home/jovyan/mi-drive/medinfo_lab/Research_Projects/zhang/Husky/data/del_none_data_for_train_tokenizer.txt'.replace(\"mi-drive\", \"nas\")\n",
    "CorpusPath = '/home/jovyan/data/del_none_data_for_train_tokenizer.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0370d299-f795-40c9-bfff-92a5c0e075aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-x 1 jovyan users 9.6G Dec  7 15:25 /home/jovyan/data/del_none_data_for_train_tokenizer.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lha $CorpusPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad694f7e-6ef8-4e5e-8a74-15c1a13427d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 10001 % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b80ceaa7-cf16-425b-b635-8fe473ef27ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/nas/medinfo_lab/Research_Projects/zhang/Husky/tools/data/MANBYO_202106.dic'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DICT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dee137b-b805-4208-b2c8-194655d10e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "# sys.path.append('/home/jovyan/mi-drive/medinfo_lab/Research_Projects/zhang/Husky')\n",
    "sys.path.append('/home/jovyan/nas/medinfo_lab/Research_Projects/zhang/Husky')\n",
    "# dependence\n",
    "import MeCab\n",
    "import json\n",
    "import ipadic\n",
    "import six\n",
    "from tools import DICT_PATH, STOP_WORDS\n",
    "\n",
    "# PATH\n",
    "SPACES_PATTERN = r\"[\\s\\n\\r]+\"\n",
    "SPACES_RE = re.compile(SPACES_PATTERN)\n",
    "\n",
    "with open(STOP_WORDS, 'r', encoding='UTF-8') as file:\n",
    "     stopwords = json.load(file)\n",
    "tagger = MeCab.Tagger(ipadic.MECAB_ARGS + f\" -O wakati {DICT_PATH}\")\n",
    "\n",
    "def tokenizex(text, stemmer):\n",
    "    text = text.lower()\n",
    "    vail_tokens = []\n",
    "    _tokens = SPACES_RE.split(text)\n",
    "    if stemmer:\n",
    "        _tokens = [six.ensure_str(stemmer.stem(x)) if len(x) > 3 else x for x in _tokens]\n",
    "    # with open(STOP_WORDS, 'r', encoding='UTF-8') as file:\n",
    "    #     stopwords = json.load(file)\n",
    "\n",
    "    # tagger = MeCab.Tagger(ipadic.MECAB_ARGS + f\" -O wakati -u {DICT_PATH}\")\n",
    "    for line in _tokens:\n",
    "        tokens = tagger.parse(line)\n",
    "        if tokens is None:\n",
    "            continue\n",
    "        else:\n",
    "            tokens = tokens.split()\n",
    "            tokens = [token for token in tokens if token not in stopwords]\n",
    "            vail_tokens.extend(tokens)\n",
    "\n",
    "    return vail_tokens\n",
    "\n",
    "\n",
    "def tokenizey(text, stemmer):\n",
    "    text = text.lower()\n",
    "    vail_tokens = []\n",
    "    _tokens = SPACES_RE.split(text)\n",
    "    if stemmer:\n",
    "        _tokens = [six.ensure_str(stemmer.stem(x)) if len(x) > 3 else x for x in _tokens]\n",
    "    # with open(STOP_WORDS, 'r', encoding='UTF-8') as file:\n",
    "    #     stopwords = json.load(file)\n",
    "\n",
    "    # tagger = MeCab.Tagger(ipadic.MECAB_ARGS + f\" -O wakati -u {DICT_PATH}\")\n",
    "    for line in _tokens:\n",
    "        tokens = tagger.parse(line)\n",
    "        if tokens is None:\n",
    "            continue\n",
    "        else:\n",
    "            tokens = tokens.split()\n",
    "            tokens = [token for token in tokens if token not in stopwords]\n",
    "            tokens = list(set(tokens))\n",
    "            vail_tokens.extend(tokens)\n",
    "\n",
    "    return vail_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9266aeb-fabb-4d5c-a8e2-049b6878019e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0798b6a-b447-4d6a-9bcc-062714e248e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d1af57a-bd66-438d-952e-2075660cce1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def lines2tokendict(lines):\n",
    "    l_tokens = []\n",
    "    for s in lines:\n",
    "        l_tokens.extend(tokenizex(text=s, stemmer=False))\n",
    "    return Counter(l_tokens)\n",
    "\n",
    "def lines2docdict(lines):\n",
    "    l_tokens = []\n",
    "    for s in lines:\n",
    "        l_tokens.extend(tokenizey(text=s, stemmer=False))\n",
    "    return Counter(l_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "352c7af8-fa0b-4b13-a732-ebe107147b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tfidf_vectorizer = TfidfVectorizer(use_idf=True, tokenizer=tokenizex, input=\"filename\")\n",
    "\n",
    "# # tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform([CorpusPath])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "181f9211-db09-46c3-8741-30e134ff6025",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 4 µs, total: 6 µs\n",
      "Wall time: 11.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# with open(CorpusPath, 'r', encoding='utf-8') as f:\n",
    "# #     for i, s_line in enumerate(f):\n",
    "# #         if (i+1) % 10000000 == 0:\n",
    "# #             print(i)\n",
    "#     # res_list = Parallel(n_jobs=-1, verbose=5, backend=\"multiprocessing\")(delayed(line2token)(l) for l in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ece768bf-542b-40c7-908e-3e3093366dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def load_data(CorpusPath):\n",
    "#     l_d = []\n",
    "#     with open(CorpusPath, 'r', encoding='utf-8') as f:\n",
    "#         l_text = []\n",
    "#         for i, l in enumerate(f):\n",
    "#             l_text.append(l)\n",
    "#             # print(l)\n",
    "#             # break\n",
    "#             if (i+1) % 1000 == 0:\n",
    "#                 l_d.append(lines2tokendict(l_text))\n",
    "\n",
    "#                 l_text = []\n",
    "                \n",
    "#             if (i+1) % 100000 == 0:\n",
    "#                 print(i)\n",
    "\n",
    "#             # if (i+1) == 3000:\n",
    "#             #     break\n",
    "#     return l_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f26b440-f077-4cad-9056-6d3948cd284f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169827322\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_line_count(filename):\n",
    "    output = subprocess.check_output([\"wc\", \"-l\", filename])\n",
    "    return int(output.decode(\"utf-8\").split()[0])\n",
    "\n",
    "len_lines_whole = get_line_count(CorpusPath)\n",
    "print(len_lines_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc4b9e20-5efc-4bb6-9dbf-4b53d48869b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.7 ms, sys: 402 µs, total: 29.1 ms\n",
      "Wall time: 27.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "IN = Path(CorpusPath)\n",
    "\n",
    "def run(start: int, end: int):\n",
    "    current = start\n",
    "    \n",
    "    l_text = []\n",
    "    l_t = []\n",
    "    l_d = []\n",
    "    \n",
    "    with IN.open() as f:\n",
    "        f.seek(start)\n",
    "\n",
    "                        \n",
    "        for i, line in enumerate(f):\n",
    "            current += len(line.encode())\n",
    "            line = line.strip()\n",
    "                \n",
    "            l_text.append(line)\n",
    "            # print(l)\n",
    "            # break\n",
    "            if (i+1) % 1000 == 0:\n",
    "                l_t.append(lines2tokendict(l_text))\n",
    "                l_d.append(lines2docdict(l_text))\n",
    "\n",
    "                l_text = []\n",
    "                #print(i)\n",
    "                \n",
    "                \n",
    "                \n",
    "            if current >= end:\n",
    "                break\n",
    "        t_whole = reduce(add, l_t)\n",
    "        d_whole = reduce(add, l_d)\n",
    "    return t_whole, d_whole\n",
    "\n",
    "\n",
    "\n",
    "file_size = IN.stat().st_size\n",
    "# num_procs = os.cpu_count()\n",
    "num_procs = 512\n",
    "\n",
    "chunk_size = file_size // num_procs\n",
    "chunks = []\n",
    "start, end = 0, chunk_size\n",
    "\n",
    "with IN.open(encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    while end < file_size:\n",
    "        f.seek(end)\n",
    "        f.readline()\n",
    "\n",
    "        end = f.tell()\n",
    "        chunks.append((start, end))\n",
    "        start, end = end, end + chunk_size\n",
    "\n",
    "    chunks.append((start, file_size))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fa14a5e-86b5-49c9-934f-8d614844c847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01b21141-ceb0-490c-b801-9066d41e0eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0a565dc-2549-43f9-8849-91ea179ff747",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   29.1s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   57.1s\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:   58.6s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 501 out of 512 | elapsed:  7.7min remaining:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done 512 out of 512 | elapsed:  7.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 5s, sys: 4.9 s, total: 4min 10s\n",
      "Wall time: 11min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_list = Parallel(n_jobs=-1, verbose=10, backend=\"multiprocessing\")(delayed(run)(i[0], i[1]) for i in chunks)\n",
    "\n",
    "l_t = []\n",
    "l_d = []\n",
    "\n",
    "for r in res_list:\n",
    "    l_t.append(r[0])\n",
    "    l_d.append(r[1])\n",
    "# with Pool(processes=num_procs) as pool:\n",
    "#     for d_sub in pool.starmap(run, chunks):\n",
    "#         res_list.append(d_sub)\n",
    "\n",
    "t_whole = reduce(add, l_t)\n",
    "d_whole = reduce(add, l_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45c81776-adf5-40f6-9688-31d801a0b42d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def d2p(d, f):\n",
    "    with open(f, 'wb') as file:\n",
    "        pickle.dump(d, file)\n",
    "\n",
    "\n",
    "def p2d(f):\n",
    "    with open(f, 'rb') as file:\n",
    "        d = pickle.load(file)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11d92a99-4d27-4cbc-8908-dd855e95e7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 393 ms, sys: 24.1 ms, total: 417 ms\n",
      "Wall time: 637 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "d2p(t_whole, \"./whole_t_count.pkl\")\n",
    "d2p(d_whole, \"./whole_d_count.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007bf098-034c-470c-a0cb-5178463f2e40",
   "metadata": {},
   "source": [
    "dict about token count in whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4dba9046-fd65-4ba6-9eff-b8f7f7ef4a96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ 8562108\n",
      "確認 2200699\n",
      "2 18866482\n",
      "nd 83273\n",
      "後 6736704\n",
      "3 13155441\n",
      "m 1878588\n",
      "以降 557666\n",
      "af 227995\n",
      "再発 1241549\n",
      "日 8616066\n"
     ]
    }
   ],
   "source": [
    "for i, (k,v) in enumerate(t_whole.items()):\n",
    "    print(k, v)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7970b-b351-4be2-a4a3-a1afe5676b1b",
   "metadata": {},
   "source": [
    "dict about document count about tokents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "903cef80-be0a-4883-a17d-67d63dd174db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ 7811666\n",
      "確認 2124621\n",
      "m 1829484\n",
      "化 1048064\n",
      "時期 165045\n",
      "3 12119719\n",
      "後 6411274\n",
      "再発 1206246\n",
      "以降 550198\n",
      "日 7764575\n",
      "大体 8729\n"
     ]
    }
   ],
   "source": [
    "for i, (k,v) in enumerate(d_whole.items()):\n",
    "    print(k, v)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc9d52-9253-4ffb-a6b7-a6e9949f62b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00661fc-bdfd-4c47-b4f1-fe690933fcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c89ed3f-e67d-4ed0-8cda-1b83306e3eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad9d80-e7cd-4761-9b3e-85c9d778293c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ad974-3ea6-4651-a6e3-5baa6577ec24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c503a90-0403-4c70-85f8-4d0bda1371e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58f94f-8a3c-46aa-b426-ef2c422d7cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b17e5baf-21ef-4950-bd24-86011ea938bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'クラリシッド': 1,\n",
       "         '下痢': 1,\n",
       "         'カルボシステイン': 1,\n",
       "         '湿疹': 1,\n",
       "         '小': 1,\n",
       "         '青': 1,\n",
       "         '竜': 1,\n",
       "         '湯': 1,\n",
       "         '胃': 1,\n",
       "         '重': 1})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = lines2tokendict([\"クラリシッドで下痢\",\"カルボシステインで湿疹\", \"小青竜湯で胃の重さ\"])\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16279817-bcf2-40dd-bfc4-883adb7a60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = lines2tokendict([\"クラリシッドでクラリシッドの下痢\",\"カルボシステインで湿疹がカルボシステインに湿疹\", \"小青竜湯で胃の重さに胃が重い\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8d60a09-b9d9-4809-b4b3-1416398f2811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d2 = lines2tokendict([\"アンブロキソールで途中覚醒\",\"カルボシステインで湿疹\", \"小青竜湯で胃の重さ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b2885-2196-46ba-b968-02bf422d7813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tools.valuations.entropy import calculate_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a574e89-fc30-4cb2-bd01-82a775993094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "sentence_matrix = tfidf_vectorizer.transform(data[:1000]).toarray()\n",
    "for index, line in enumerate(data[:1000]):\n",
    "    ens = calculate_entropy(sentence_matrix, batch=True)\n",
    "    output.append(ens[index], line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a548462-1e6c-4bac-a8c1-10755be998f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "entropy = []\n",
    "for item in output:\n",
    "    entropy.append(item[0])\n",
    "plt.hist(entropy, bins=80)\n",
    "plt.xlabel('entropy')\n",
    "plt.ylabel('count')\n",
    "plt.title('entropy distribute Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d56717-e055-47b4-aafe-b0b36ab83574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
